function out = fit_semi_pairwise(data, in, options)
%fit_semi_pairwise Fits semiparametric pairwise model to data. 
%This function uses gradient descent on negative log-likelihood with
%gradient estimated by averaging over samples from the model. Samples at
%T+1th iteration are generated by applying the MCMC transition matrix to
%samples at Tth iteration.
%
% Syntax: out = fit_semi_pairwise(data, in, options)
%
% Inputs:
%   data: Binary array of size number of samples x number of neurons.
%   in: Struct with fields corresponding to initial parameters.
%     .J: Initial guess for J.
%     .x: Initial guess for the parameters of V, see monotone.m.
%     .bin_centers: Hyperparameter of V, see monotone.m.
%   options: Struct with fields corresponding to hyperparameters.
%     .learning_rate_J: Learning rate for J.
%     .learning_rate_x: Learning rate for x.
%     .iter: Number of gradient descent iterations.
%     .M_samples: Number of samples used for estimating gradient.
%     .gibbs_steps: Number of single neuron flips that take you from 
%                   samples at Tth iteration to samples at T+1th iteration.
%
% Outputs:
%   out: Struct with fields corresponding to learned parameters.
%     .J: Learned J.
%     .x: Learned parameters of V, see monotone.m.
%     .bin_centers: Hyperparameter of V, same as in "in", see monotone.m.
%
% Required m-files: monotone.m, DmonotoneDt.m, sample_semi_pairwise.m,
%                   grad_descent.m

% Initialize
n = size(data,2);
x0 = in.x;
bin_centers = in.bin_centers;
J0 = in.J;
pars0 = [J0(:);x0];
learning_rate = [ones(size(J0(:)))*options.learning_rate_J; ...
                 ones(size(x0))*options.learning_rate_x];
% Initialize gibbs chain at data distribution.
samples = data(randi(length(data), options.M_samples, 1),:);
% Start a parallel pool.
pool = gcp;
n_pools = pool.NumWorkers;
% Divide both data and samples into batches to be processed in parallel.
data = data(1:(floor(length(data)/n_pools)*n_pools),:);
samples = samples(1:(floor(length(samples)/n_pools)*n_pools),:);
M_data = size(data, 1);
M_samples = size(samples, 1);
samples_batch = zeros(M_samples/n_pools, n, n_pools);
data_batch = zeros(M_data/n_pools, n, n_pools);
for k = 1:n_pools
    idx_samples = ((k-1)*(M_samples/n_pools) + 1):(k*M_samples/n_pools);
    samples_batch(:,:,k) = samples(idx_samples,:);
    idx_data = ((k-1)*(M_data/n_pools) + 1):(k*M_data/n_pools);
    data_batch(:,:,k) = data(idx_data,:);
end
% Transform samples into samples from the initial model.
V = @(E)monotone(x0', bin_centers, E);
burn_in = 10*options.gibbs_steps; % Rough guess for MCMC burn-in time.
parfor k = 1:n_pools
    samples = squeeze(samples_batch(:,:,k));
    samples = sparse(samples);
    samples_batch(:,:,k) = sample_semi_pairwise(samples, J0, V, burn_in);
end
% Minimize negative log-likelihood.
grad = @(pars, samples_batch)Dloss(pars, bin_centers, data_batch, ...
                                   samples_batch, options.gibbs_steps, ...
                                   n_pools);
pars = grad_descent(grad, pars0, learning_rate, samples_batch, ...
                    options.iter);
out.J = reshape(pars(1:n^2), [n,n]);
out.x = pars((n^2+1):end);
out.bin_centers = in.bin_centers;

end


function [Dloss, samples_batch] = Dloss(pars, bin_centers, data_batch, ...
                                        samples_batch, gibbs_steps, ...
                                        n_pools)
%Executes gradient estimation over several batches in parallel. Actual
%gradient estimation happens in "Dloss_per_batch" below.

Dloss_batch = zeros(length(pars), n_pools);
parfor k = 1:n_pools
    data = squeeze(data_batch(:,:,k));
    samples = squeeze(samples_batch(:,:,k));
    samples = sparse(samples);
    data = sparse(data);
    [Dloss_batch(:,k), samples] = Dloss_per_batch(pars, bin_centers, ...
                                                  data, samples, ...
                                                  gibbs_steps);
    samples_batch(:,:,k) = samples;
end
Dloss = sum(Dloss_batch, 2)/n_pools;

end


function [Dloss_per_batch, samples] = Dloss_per_batch(pars, bin_centers,...
                                                      data, samples,...
                                                      gibbs_steps)
%Samples the current model with parameters "pars" by applying MCMC to past
%samples in "samples", and uses these to estimate the loss gradient at the
%current model.

% Initialize.
n = size(data,2);
J = reshape(pars(1:n^2), [n,n]);
x = pars((n^2+1):end);
V = @(E)monotone(x', bin_centers, E);
% Estimate model part of gradient wrt V parameters.
samples = sample_semi_pairwise(samples, J, V, gibbs_steps);
E_samples = sum(samples.*(samples*J), 2);
[tmp, DV_model] = monotone(x', bin_centers, E_samples);
DV_model = mean(DV_model, 1)';
% Estimate model part of gradient wrt J.
dVdE = @(E)DmonotoneDt(x', bin_centers, E);
DJ_model = samples'*bsxfun(@times, dVdE(E_samples),...
                           samples)/length(samples);
% Data part of gradient wrt V parameters.
E_data = sum(data.*(data*J), 2);
[tmp, DV_data] = monotone(x', bin_centers, E_data);
DV_data = mean(DV_data, 1)';
% Data part of gradient wrt J.
DJ_data = data'*bsxfun(@times, dVdE(E_data), data)/length(data);
% Total gradient.
Dloss_per_batch = [-DJ_model(:) + DJ_data(:); -DV_model + DV_data];

end