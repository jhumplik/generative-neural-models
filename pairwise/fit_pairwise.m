function J = fit_pairwise(data, J0, options)
%fit_pairwise Fits pairwise model to data. 
%This function uses gradient descent on negative log-likelihood with
%gradient estimated by averaging over samples from the model. Samples at
%T+1th iteration are generated by applying the MCMC transition matrix to
%samples at Tth iteration.
%
% Syntax: J = fit_pairwise(data, J0, options)
%
% Inputs:
%   data: Binary array of size number of samples x number of neurons.
%   J0: Initial guess for J.
%   options: Struct with fields corresponding to hyperparameters.
%     .learning_rate: Learning rate for gradient descent.
%     .iter: Number of gradient descent iterations.
%     .M_samples: Number of samples used for estimating gradient.
%     .gibbs_steps: Number of single neuron flips that take you from 
%                   samples at Tth iteration to samples at T+1th iteration.
%
% Outputs:
%   J: Learned coupling matrix.
%
% Required m-files: sample_pairwise.m, grad_descent.m

% Initialize
[M,n] = size(data);
J0_lin = J0(:);
% Estimate empirical covariance which is to be reproduced by model.
emp_cov = (data'*data)/M;
% Initialize gibbs chain at data distribution.
samples = data(randi(length(data), options.M_samples, 1),:);
% Start a parallel pool.
pool = gcp;
n_pools = pool.NumWorkers;
% Divide both samples into batches to be processed in parallel.
samples = samples(1:(floor(length(samples)/n_pools)*n_pools),:);
M_samples = size(samples, 1);
samples_batch = zeros(M_samples/n_pools, n, n_pools);
for k = 1:n_pools
    idx_samples = ((k-1)*(M_samples/n_pools) + 1):(k*M_samples/n_pools);
    samples_batch(:,:,k) = samples(idx_samples,:);
end
% Transform samples into samples from the initial model.
burn_in = 10*options.gibbs_steps; % Rough guess for MCMC burn-in time.
parfor k = 1:n_pools
    samples = squeeze(samples_batch(:,:,k));
    samples = sparse(samples);
    samples_batch(:,:,k) = sample_pairwise(samples, J0, burn_in);
end
% Minimize negative log-likelihood.
grad = @(J_lin, samples_batch)Dloss(J_lin, data, emp_cov, ...
                                    samples_batch, options.gibbs_steps, ...
                                    n_pools);
J_lin = grad_descent(grad, J0_lin, options.learning_rate, ...
                     samples_batch, options.iter);
J = reshape(J_lin, [n,n]);
end


function [Dloss, samples_batch] = Dloss(J_lin, data, emp_cov, ...
                                        samples_batch, gibbs_steps, ...
                                        n_pools)
%Samples the current model J by applying MCMC to past samples in 
%"samples_batch", and uses these to estimate the loss gradient at the
%current model. Processes several batches in parallel.

% Initialize.
n = size(data, 2);
J = reshape(J_lin, [n,n]);
% Draw samples and estimate model covariance matrix
model_covs = zeros(n^2, n_pools);
parfor k = 1:n_pools
    samples = squeeze(samples_batch(:,:,k));
    samples = sparse(samples);
    samples = sample_pairwise(samples, J, gibbs_steps);
    model_cov_tmp = samples'*samples/length(samples);
    model_covs(:,k) = model_cov_tmp(:);
    samples_batch(:,:,k) = samples;
end
model_cov = sum(model_covs, 2)/n_pools;
% Calculate gradient.
Dloss = (-model_cov + emp_cov(:));
end